---
title: "XGBOOST FIT"
author: "Victor M. Uribe"
date: "2023-01-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(tidymodels)
library(finetune)
```








```{r}
xgb_spec <- boost_tree(
  mtry = tune(),
  min_n = tune(),
  trees = tune(),
  loss_reduction = tune(),
  learn_rate = tune()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")


xgb_rec <- recipe(success ~., everest_train) %>% 
  step_other(expedition_role, citizenship, threshold = .05) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors())


xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(xgb_rec)
```




```{r}
doParallel::registerDoParallel()
set.seed(123)


xgb_para <- parameters(
  finalize(mtry(), everest_train),
  trees(),
  loss_reduction(),
  learn_rate(),
  min_n()#,
  #finalize(sample_size(), everest_train)
)


xgb_grid <- grid_max_entropy(
  xgb_para,
  size = 20
)


xgb_tune <- tune_race_anova(
  xgb_wf,
  resamples = everest_fold,
  grid = xgb_grid,
  metrics = metric_set(roc_auc, sensitivity, specificity),
  control = control_race(verbose_elim = TRUE)
)

```


```{r}
xgb_tune %>% autoplot()
```


```{r}
best_xgb <- xgb_tune %>% 
  select_best("roc_auc")

fin_xgb_wf <- finalize_workflow(
  xgb_wf,
  best_xgb
)
  

fit_xgb <- fin_xgb_wf %>% 
  fit(everest_train)
```





```{r}
doParallel::registerDoParallel()


xgb_pred <- fit_xgb %>% 
  predict(new_data = everest_test) 


xgb_tmp <- factor(ifelse(xgb_pred == TRUE, TRUE, FALSE)) 
caret::confusionMatrix(xgb_tmp, everest_test[["success"]])
```






|     Roc Auc     | Accuracy | Sensitivity | Specificity
|---              |---       |---          |---   
|  Lasso          |  0.8703  |    0.7939   |  0.9554     
|  Random Forest  |  0.8756  |    0.8156   |  0.9423
|  xgboost        |  0.8733  |    0.8132   |  0.9403














